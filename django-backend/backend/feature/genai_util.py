import os
from dotenv import load_dotenv
import vertexai
import json
from vertexai.generative_models import GenerativeModel

load_dotenv() #load env variables

# --- Gemini API Configurations ---
file_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

try:
    with open(file_path, 'r', encoding='utf-8') as file:
        data = json.load(file)
        project_id = data['project_id']
        location = 'us-central1'
except FileNotFoundError:
    print(f"File not found: {file_path}")

model_name = os.getenv("GEMINI_MODEL")

# Initializing Vertex AI SDK using the loaded config
vertexai.init(project=project_id, location=location) 

gemini_model = GenerativeModel(model_name=model_name)


def extract_text_from_document(input_dir, document):
    """Extract and concatenate all text from the document."""
    doc_path = os.path.join(os.path.join(input_dir, "temp_files"), document.replace('.pdf', '.json'))
    print(doc_path)

    if not os.path.exists(doc_path) or not doc_path.endswith('.json'):
        raise FileNotFoundError(f"File Error")

    with open(doc_path, 'r', encoding='utf-8') as f:
        try:
            data = json.load(f).get('full_text', [])
        except json.JSONDecodeError as e:
            raise ValueError(f"Error decoding JSON in {doc_path}: {e}")

    text_parts = ""
    for item in data:
        text_parts += item

    return text_parts


def query_llm(prompt):
    """Query the Gemini LLM API with the given prompt."""
    try:
        # Load the model
        # Generate content
        response = gemini_model.generate_content(
            prompt,
            generation_config={
                "temperature": 0.5,
                "max_output_tokens": 2000
            }
        )

        # Extract generated text
        if response.candidates and len(response.candidates) > 0:
            return response.candidates[0].content.parts[0].text.strip()
        else:
            print("No valid response generated.")
            return None

    except Exception as e:
        print(f"API request failed: {e}")
        return None


def generate_prompts_enhanced(text):
  """
  Generate prompts for key insights, facts, and counterpoints,
  ensuring a response is always generated by using the internet if the
  source text is insufficient.
  """
  prompts = {
      "key_insights": (
          "Extract 3-5 key insights from the following text. "
          "If the text itself does not contain enough information, research the topic of the text to derive the insights. "
          "Present them as a python list of strings without any extra text or backticks:\n\n" + text
      ),
      "did_you_know": (
          "Extract 2-3 interesting 'Did You Know?' facts from the following text. "
          "If the text does not contain any, use the internet to find interesting facts related to the text's main subject. "
          "Present them as a python list of strings without any extra text or backticks:\n\n" + text
      ),
      "counterpoints": (
          "Identify 2-3 potential counterpoints or opposing views to the main arguments in the following text. "
          "If the text does not explicitly mention any, infer them or use the internet to find common counterarguments to the points made. "
          "Present them as a python list of strings without any extra text or backticks:\n\n" + text
      )
  }
  return prompts


def process_document(input, document):
    """Process a document to extract insights."""
    text = extract_text_from_document(input, document)
    if not text.strip():
        raise ValueError("No text found in document")

    prompts = generate_prompts_enhanced(text)
    results = {}

    # Process each prompt sequentially
    for key, prompt in prompts.items():
        print(f"Processing {key.replace('_', ' ')}...")
        results[key] = query_llm(prompt)

    return results, text


def main():
    input_dir = "/content/sample_data"
    file_name = "South_of_France_-_History.pdf"

    try:
        results = process_document(input_dir, file_name)

        for key, value in results.items():
            print(value or "No response from LLM.")

    except Exception as e:
        print(f"Error: {e}")


if __name__ == "__main__":
    main()

 